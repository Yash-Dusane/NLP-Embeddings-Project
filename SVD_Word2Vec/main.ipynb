{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 26753\n",
      "Sparse Co-occurrence Matrix is built.\n",
      "Truncated SVD ...\n",
      "Embeddings saved to svd.pt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import re\n",
    "import collections\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_corpus():\n",
    "    corpus = brown.sents()\n",
    "    processed_corpus = []\n",
    "    for sentence in corpus:\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        sentence = [re.sub(r'[^a-z]', '', word) for word in sentence]\n",
    "        sentence = [word for word in sentence if word]\n",
    "        if sentence:\n",
    "            processed_corpus.append(sentence)\n",
    "    return processed_corpus\n",
    "\n",
    "def build_vocabulary(corpus, min_freq=2):\n",
    "    word_counts = collections.Counter(word for sentence in corpus for word in sentence)\n",
    "    vocab = {}\n",
    "    vocab[\"<UNK>\"] = 0\n",
    "    idx = 1\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "tokenized_corpus = tokenize_corpus()\n",
    "vocab = build_vocabulary(tokenized_corpus)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "embedding_dim = 200\n",
    "\n",
    "word_to_id = {word: i for i, word in enumerate(sorted(vocab))}\n",
    "id_to_word = {i: word for word, i in word_to_id.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "co_occurrence = defaultdict(Counter)\n",
    "for sentence in tokenized_corpus:\n",
    "    sentence = [word.lower() for word in sentence if word in vocab]\n",
    "    for i, target in enumerate(sentence):\n",
    "        for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "            if i != j:\n",
    "                co_occurrence[target][sentence[j]] += 1\n",
    "\n",
    "rows, cols, data = [], [], []\n",
    "for word, context_words in co_occurrence.items():\n",
    "    for context_word, count in context_words.items():\n",
    "        rows.append(word_to_id[word])\n",
    "        cols.append(word_to_id[context_word])\n",
    "        data.append(count)\n",
    "\n",
    "co_matrix = coo_matrix((data, (rows, cols)), shape=(vocab_size, vocab_size), dtype=np.float64)\n",
    "print(\"Sparse Co-occurrence Matrix is built.\")\n",
    "\n",
    "print(\"Truncated SVD ...\")\n",
    "U, S, Vt = svds(co_matrix, k=embedding_dim)\n",
    "U = U[:, ::-1]\n",
    "S = S[::-1]\n",
    "Vt = Vt[::-1, :]\n",
    "\n",
    "embeddings = U * np.sqrt(S)\n",
    "\n",
    "embedding_dict = {\n",
    "    \"word_to_id\": word_to_id,\n",
    "    \"word_vectors\": embeddings\n",
    "}\n",
    "torch.save(embedding_dict, \"svd.pt\")\n",
    "\n",
    "print(\"Embeddings saved to svd.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 26753\n",
      "Loaded vocabulary size: 26753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15693/15693 [01:42<00:00, 152.40batch/s, loss=2.5152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 2.0690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15693/15693 [01:41<00:00, 155.14batch/s, loss=1.8429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 1.5950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15693/15693 [01:42<00:00, 153.75batch/s, loss=1.7058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 1.2931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 15693/15693 [01:41<00:00, 154.48batch/s, loss=0.9544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Loss: 1.0798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 15693/15693 [01:40<00:00, 155.78batch/s, loss=1.2062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 0.9307\n",
      "CBOW embeddings saved in the required format as cbow_manual.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "def tokenize_corpus():\n",
    "    corpus = brown.sents()\n",
    "    processed_corpus = []\n",
    "    for sentence in corpus:\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        sentence = [re.sub(r'[^a-z]', '', word) for word in sentence]\n",
    "        sentence = [word for word in sentence if word]\n",
    "        if sentence:\n",
    "            processed_corpus.append(sentence)\n",
    "    return processed_corpus\n",
    "\n",
    "def build_vocabulary(corpus, min_freq=2):\n",
    "    word_counts = collections.Counter(word for sentence in corpus for word in sentence)\n",
    "    vocab = {}\n",
    "    vocab[\"<UNK>\"] = 0\n",
    "    idx = 1\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "tokenized_corpus = tokenize_corpus()\n",
    "vocab = build_vocabulary(tokenized_corpus)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "\n",
    "word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "id_to_word = {i: word for word, i in word_to_id.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# word_to_id = vocab\n",
    "# idx2word = {idx: word for word, idx in word_to_id.items()}\n",
    "print(f\"Loaded vocabulary size: {vocab_size}\")\n",
    "\n",
    "\n",
    "embedding_dim = 100\n",
    "window_size = 2\n",
    "num_negative = 10\n",
    "num_epochs = 5\n",
    "learning_rate = 0.005\n",
    "batch_size = 128\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, sentences, word_to_id, window_size):\n",
    "        self.examples = []\n",
    "        for sentence in sentences:\n",
    "            indices = [word_to_id.get(word, word_to_id.get(\"<UNK>\")) for word in sentence]\n",
    "            for i, target in enumerate(indices):\n",
    "                context = []\n",
    "                for j in range(max(0, i - window_size), min(len(indices), i + window_size + 1)):\n",
    "                    if j != i:\n",
    "                        context.append(indices[j])\n",
    "                if context:\n",
    "                    self.examples.append((context, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    contexts = [torch.tensor(item[0], dtype=torch.long) for item in batch]\n",
    "    targets = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
    "    contexts_padded = pad_sequence(contexts, batch_first=True, padding_value=-1)\n",
    "    lengths = torch.tensor([len(x) for x in contexts], dtype=torch.float)\n",
    "    return contexts_padded.to(device), targets.to(device), lengths.to(device)\n",
    "\n",
    "dataset = CBOWDataset(tokenized_corpus, word_to_id, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Xavier Initialization\n",
    "\n",
    "W = torch.empty(vocab_size, embedding_dim, device=device)\n",
    "torch.nn.init.xavier_uniform_(W)\n",
    "W.requires_grad = True\n",
    "\n",
    "C = torch.empty(vocab_size, embedding_dim, device=device)\n",
    "torch.nn.init.xavier_uniform_(C)\n",
    "C.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([W, C], lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "def sample_negative(batch_targets, num_negative, vocab_size):\n",
    "    B = batch_targets.size(0)\n",
    "    # Generate random indices in [0, vocab_size-2]\n",
    "    neg_samples = torch.randint(0, vocab_size-1, (B, num_negative), device=device)\n",
    "    # Shift indices >= target to exclude the target value\n",
    "    mask = neg_samples >= batch_targets.unsqueeze(1)\n",
    "    neg_samples += mask.long()  # Add 1 to masked positions\n",
    "    return neg_samples\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    sample_count = 0\n",
    "    epoch_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "    for contexts, targets, lengths in epoch_bar:\n",
    "        context_embeds = W[contexts]\n",
    "        mask = (contexts != -1).unsqueeze(-1).float()\n",
    "        masked_context_embeds = context_embeds * mask\n",
    "        sum_context = masked_context_embeds.sum(dim=1)\n",
    "        avg_context = sum_context / lengths.unsqueeze(1)\n",
    "        pos_scores = (avg_context * C[targets]).sum(dim=1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_scores) + 1e-10)\n",
    "        neg_samples = sample_negative(targets, num_negative, vocab_size)\n",
    "        neg_embeds = C[neg_samples]\n",
    "        neg_scores = torch.bmm(neg_embeds, avg_context.unsqueeze(2)).squeeze(2)\n",
    "        neg_loss = -torch.log(torch.sigmoid(-neg_scores) + 1e-10).sum(dim=1)\n",
    "        loss = (pos_loss + neg_loss).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        sample_count += 1\n",
    "        epoch_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    scheduler.step()\n",
    "    avg_epoch_loss = total_loss / sample_count if sample_count > 0 else 0\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "word_vectors = (W + C).detach().cpu().numpy()\n",
    "\n",
    "embedding_dict = {\n",
    "    \"word_to_id\": word_to_id,\n",
    "    \"word_vectors\": word_vectors\n",
    "}\n",
    "torch.save(embedding_dict, \"cbow_manual.pt\")\n",
    "print(\"CBOW embeddings saved in the required format as cbow.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 26753\n",
      "Loaded vocabulary size: 26753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 57517/57517 [05:31<00:00, 173.74batch/s, loss=2.8274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 2.5265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 57517/57517 [05:19<00:00, 180.26batch/s, loss=2.9136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 2.3932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 57517/57517 [05:25<00:00, 176.62batch/s, loss=2.1991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 2.2918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 57517/57517 [05:17<00:00, 181.12batch/s, loss=2.5104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Loss: 2.2151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 57517/57517 [05:11<00:00, 184.38batch/s, loss=2.6108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 2.1546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 57517/57517 [05:18<00:00, 180.74batch/s, loss=1.5990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Loss: 2.0924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 57517/57517 [05:18<00:00, 180.82batch/s, loss=2.0380]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Loss: 2.0277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 57517/57517 [05:19<00:00, 180.25batch/s, loss=1.4837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Average Loss: 1.9636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 57517/57517 [05:16<00:00, 181.50batch/s, loss=2.1201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Average Loss: 1.8999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 57517/57517 [05:26<00:00, 176.41batch/s, loss=2.3489]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 1.8459\n",
      "Skip-gram embeddings saved as skipgram_manual.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "from nltk.corpus import brown\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def tokenize_corpus():\n",
    "    corpus = brown.sents()\n",
    "    processed_corpus = []\n",
    "    for sentence in corpus:\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        sentence = [re.sub(r'[^a-z]', '', word) for word in sentence]\n",
    "        sentence = [word for word in sentence if word]\n",
    "        if sentence:\n",
    "            processed_corpus.append(sentence)\n",
    "    return processed_corpus\n",
    "\n",
    "def build_vocabulary(corpus, min_freq=2):\n",
    "    word_counts = collections.Counter(word for sentence in corpus for word in sentence)\n",
    "\n",
    "    vocab = {}\n",
    "    vocab[\"<UNK>\"] = 0\n",
    "    idx = 1\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "tokenized_corpus = tokenize_corpus()\n",
    "vocab = build_vocabulary(tokenized_corpus)\n",
    "\n",
    "word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "id_to_word = {i: word for word, i in word_to_id.items()}\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "\n",
    "embedding_dim = 100\n",
    "window_size = 2\n",
    "num_negative = 20\n",
    "num_epochs = 10\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, sentences, word_to_id, window_size):\n",
    "        self.examples = []\n",
    "        for sentence in sentences:\n",
    "            indices = [word_to_id.get(word, word_to_id[\"<UNK>\"]) for word in sentence]\n",
    "            for i, target in enumerate(indices):\n",
    "                start = max(0, i - window_size)\n",
    "                end = min(len(indices), i + window_size + 1)\n",
    "                for j in range(start, end):\n",
    "                    if j != i:\n",
    "                        context = indices[j]\n",
    "                        self.examples.append((target, context))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    targets = torch.tensor([item[0] for item in batch], dtype=torch.long)\n",
    "    contexts = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
    "    return targets.to(device), contexts.to(device)\n",
    "\n",
    "dataset = SkipGramDataset(tokenized_corpus, word_to_id, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "W = torch.empty(vocab_size, embedding_dim, device=device)\n",
    "torch.nn.init.xavier_uniform_(W)\n",
    "W.requires_grad = True\n",
    "\n",
    "C = torch.empty(vocab_size, embedding_dim, device=device)\n",
    "torch.nn.init.xavier_uniform_(C)\n",
    "C.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([W, C], lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "def sample_negative(batch_targets, num_negative, vocab_size):  # Fixed: use targets\n",
    "    B = batch_targets.size(0)\n",
    "    neg_samples = torch.randint(0, vocab_size-1, (B, num_negative), device=device)\n",
    "    mask = neg_samples >= batch_targets.unsqueeze(1)\n",
    "    neg_samples += mask.long()\n",
    "    return neg_samples\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    sample_count = 0\n",
    "    epoch_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "    for targets, contexts in epoch_bar:\n",
    "        pos_embeds = W[targets]\n",
    "        pos_scores = (pos_embeds * C[contexts]).sum(dim=1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_scores) + 1e-10).mean()\n",
    "\n",
    "        neg_samples = sample_negative(targets, num_negative, vocab_size)  # Pass targets\n",
    "        neg_embeds = C[neg_samples]\n",
    "        neg_scores = (pos_embeds.unsqueeze(1) * neg_embeds).sum(dim=2)\n",
    "        neg_loss = -torch.log(torch.sigmoid(-neg_scores) + 1e-10).sum(dim=1).mean()  # Sum over negatives\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        sample_count += 1\n",
    "        epoch_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    scheduler.step()\n",
    "    avg_epoch_loss = total_loss / sample_count if sample_count > 0 else 0\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "word_vectors = (W + C).detach().cpu().numpy()\n",
    "\n",
    "embedding_dict = {\n",
    "    \"word_to_id\": word_to_id,\n",
    "    \"word_vectors\": word_vectors\n",
    "}\n",
    "torch.save(embedding_dict, \"skipgram_manual.pt\")\n",
    "\n",
    "print(\"Skip-gram embeddings saved as skipgram_manual.pt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
