{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fncQVaEaICp",
        "outputId": "46006451-cb45-4def-ac04-5d8ac1f6d4ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "nltk.download('punkt_tab')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDkI19PWaNcK"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "     # Remove URLs\n",
        "    text = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Remove HTML/XML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Remove chapter headings (e.g., \"Chapter 1\", \"CHAPTER ONE\", \"CH 1\")\n",
        "    text = re.sub(r'\\b(chapter|ch)\\b[\\s\\divx]+', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove copyright and legal notices (e.g., \"Copyright © 2023\", \"All rights reserved\")\n",
        "    text = re.sub(r'\\b(copyright|©|all rights reserved|no part of this book)\\b.*', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove long sequences of digits (e.g., page numbers or codes)\n",
        "    text = re.sub(r'\\b\\d{4,}\\b', '', text)\n",
        "\n",
        "    # Remove special characters, numbers, and punctuation except for basic ones\n",
        "    text = re.sub(r'[^\\w\\s.,!?]', ' ', text)\n",
        "\n",
        "    text = text.replace(\"_\", \" \")\n",
        "    text = text.replace(\"--\", \" \")\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenizer(sentences, n):\n",
        "    tokens = []\n",
        "    padding = [\"<s>\"] * (n - 1)\n",
        "    for sentence in sentences:\n",
        "        sentence = clean_text(sentence.lower())\n",
        "        tokens += padding + nltk.word_tokenize(sentence) + [\"</s>\"]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "N = 3\n",
        "corpus_path =  \"/content/Ulysses - James Joyce.txt\"\n",
        "# corpus_path =   \"/content/Pride and Prejudice - Jane Austen.txt\"\n",
        "\n",
        "# Read the raw corpus\n",
        "with open(corpus_path, 'r', encoding='utf-8') as file:\n",
        "    raw_corpus = file.read()\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = nltk.sent_tokenize(raw_corpus)\n",
        "\n",
        "# Choose the step size for selecting test sentences (e.g., every 10th sentence)\n",
        "step_size = len(sentences) // 1000  # Ensures 1000 sentences are selected\n",
        "test_sentences = sentences[::step_size][:1000]  # Take every `step_size`-th sentence and limit to 1000\n",
        "\n",
        "# Use the remaining sentences for training\n",
        "train_sentences = [sentence for i, sentence in enumerate(sentences) if i % step_size != 0]\n",
        "\n",
        "# Tokenize the train sentences with the specified N-gram size\n",
        "tokens = tokenizer(train_sentences, N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGKpMt_FaTD3"
      },
      "outputs": [],
      "source": [
        "word_counts = Counter(tokens)\n",
        "\n",
        "# Step 2: Identify words that occur only once\n",
        "unique_words = [word for word, count in word_counts.items() if count == 1]\n",
        "\n",
        "# Step 3: Select the least frequent 5% of words as <UNK>\n",
        "num_unk = max(1, int(len(word_counts) * 0.05))  # Ensure at least 1 word is selected\n",
        "\n",
        "# Sort words by frequency (ascending order) to get the least frequent words\n",
        "sorted_by_frequency = sorted(word_counts.items(), key=lambda x: x[1])\n",
        "unk_words = [word for word, count in sorted_by_frequency[:num_unk]]\n",
        "\n",
        "# Step 4: Build vocabulary, replacing selected words with <UNK>\n",
        "vocab = {\"<UNK>\": 0}  # Start with <UNK>\n",
        "for word in word_counts.keys():\n",
        "    if word not in unk_words:\n",
        "        vocab[word] = len(vocab)  # Assign index\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Step 6: Generate trigrams\n",
        "def generate_trigrams(tokens):\n",
        "    return [([tokens[i], tokens[i+1]], tokens[i+2]) for i in range(len(tokens) - 2)]\n",
        "\n",
        "def generate_5grams(tokens):\n",
        "    return [([tokens[i], tokens[i+1], tokens[i+2], tokens[i+3]], tokens[i+4]) for i in range(len(tokens) - 4)]\n",
        "\n",
        "dataset_trigram = generate_trigrams(tokens)\n",
        "dataset_5gram = generate_5grams(tokens)\n",
        "\n",
        "# Step 7: Convert words to indices, replacing rare words with <UNK>\n",
        "def encode_dataset_3(dataset, vocab, unk_words):\n",
        "    encoded = []\n",
        "    for (w1, w2), w3 in dataset:\n",
        "        # Replace rare words with <UNK>\n",
        "        w1 = w1 if w1 not in unk_words else \"<UNK>\"\n",
        "        w2 = w2 if w2 not in unk_words else \"<UNK>\"\n",
        "        w3 = w3 if w3 not in unk_words else \"<UNK>\"\n",
        "        encoded.append(([vocab[w1], vocab[w2]], vocab[w3]))\n",
        "    return encoded\n",
        "\n",
        "def encode_dataset_5(dataset, vocab, unk_words):\n",
        "    encoded = []\n",
        "    for (w1, w2, w3, w4), w5 in dataset:\n",
        "        # Replace rare words with <UNK>\n",
        "        w1 = w1 if w1 not in unk_words else \"<UNK>\"\n",
        "        w2 = w2 if w2 not in unk_words else \"<UNK>\"\n",
        "        w3 = w3 if w3 not in unk_words else \"<UNK>\"\n",
        "        w4 = w4 if w4 not in unk_words else \"<UNK>\"\n",
        "        w5 = w5 if w5 not in unk_words else \"<UNK>\"\n",
        "        encoded.append(([vocab[w1], vocab[w2], vocab[w3], vocab[w4]], vocab[w5]))\n",
        "    return encoded\n",
        "\n",
        "# encoded_sequences = encode_sequence(sequences, vocab, unk_words)\n",
        "encoded_dataset_3 = encode_dataset_3(dataset_trigram, vocab, unk_words)\n",
        "encoded_dataset_5 = encode_dataset_5(dataset_5gram, vocab, unk_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyfCwcEma1qw",
        "outputId": "6c706e3f-9668-44fc-84db-792349be60bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 5.0741\n",
            "Epoch 2, Loss: 4.4240\n",
            "Epoch 3, Loss: 4.1693\n",
            "Epoch 4, Loss: 3.9930\n",
            "Epoch 5, Loss: 3.8557\n"
          ]
        }
      ],
      "source": [
        "class TrigramDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, target = self.data[idx]\n",
        "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "class FFNNLanguageModelTrigram(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=100, hidden_size=50):\n",
        "        \"\"\"\n",
        "        A simple feed-forward neural network language model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary.\n",
        "            embed_size (int): Dimension of the embedding vectors.\n",
        "            hidden_size (int): Number of neurons in the hidden layer.\n",
        "        \"\"\"\n",
        "        super(FFNNLanguageModelTrigram, self).__init__()\n",
        "        # Embedding layer converts word indices to vectors.\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        # For a trigram model, the context is 2 words (hence embed_size * 2)\n",
        "        self.fc1 = nn.Linear(embed_size * 2, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        # Output layer produces a score for each word in the vocabulary.\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Tensor of shape (batch_size, context_length)\n",
        "                              where context_length = 2 for trigrams.\n",
        "        Returns:\n",
        "            torch.Tensor: Output logits of shape (batch_size, vocab_size).\n",
        "        \"\"\"\n",
        "        # Get embeddings for each word in the context.\n",
        "        embeds = self.embedding(x)              # Shape: (batch_size, 2, embed_size)\n",
        "        embeds = embeds.view(embeds.size(0), -1)  # Flatten to (batch_size, 2 * embed_size)\n",
        "        hidden = self.relu(self.fc1(embeds))      # Hidden layer with ReLU activation\n",
        "        output = self.fc2(hidden)                 # Output logits for each vocabulary word\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_dataset = TrigramDataset(encoded_dataset_3)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = FFNNLanguageModelTrigram(vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for context, target in train_loader:\n",
        "        context, target = context.to(device), target.to(device)  # Move to GPU\n",
        "        optimizer.zero_grad()\n",
        "        output = model(context)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YCNm_ddgTO9"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"PP_FFNN_3.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d5iHJoXc0aj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "# Make sure necessary downloads are done\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# Assume model and vocab are already defined, and device is set:\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = ... (loaded or defined and set to eval mode)\n",
        "model.eval()\n",
        "N = 3\n",
        "\n",
        "def calculate_trigram_log_probability(model, trigram, vocab):\n",
        "    \"\"\"\n",
        "    Compute the log probability for a single trigram.\n",
        "    trigram: a tuple of three tokens, e.g. (\"the\", \"cat\", \"sat\")\n",
        "    \"\"\"\n",
        "    # Since trigram is exactly 3 tokens, we can directly unpack:\n",
        "    w1, w2, w3 = trigram\n",
        "\n",
        "    # Replace OOV words with <UNK>\n",
        "    w1 = w1 if w1 in vocab else \"<UNK>\"\n",
        "    w2 = w2 if w2 in vocab else \"<UNK>\"\n",
        "    w3 = w3 if w3 in vocab else \"<UNK>\"\n",
        "\n",
        "    # Create context tensor from the first two words\n",
        "    context_tensor = torch.tensor([[vocab[w1], vocab[w2]]], dtype=torch.long).to(device)\n",
        "\n",
        "    # Forward pass: add batch dimension\n",
        "    output = model(context_tensor.unsqueeze(0))  # shape: (1, vocab_size)\n",
        "\n",
        "    # Convert logits to log probabilities\n",
        "    log_probs = F.log_softmax(output, dim=1)\n",
        "\n",
        "    # Get log probability for the actual next word\n",
        "    target_idx = vocab[w3]\n",
        "    word_log_prob = log_probs[0, target_idx].item()\n",
        "\n",
        "    return word_log_prob\n",
        "\n",
        "# Example usage: compute perplexity for each sentence in train_sentences\n",
        "perplexities = []\n",
        "for sentence in train_sentences:\n",
        "    # Sentence may need to be split into sentences if it contains multiple sentences:\n",
        "    sentence_list = nltk.sent_tokenize(sentence)\n",
        "    # For simplicity, let's assume each 'sentence' is processed separately\n",
        "    for sent in sentence_list:\n",
        "        tokens = tokenizer([sent], N)  # tokenizer expects a list of sentences\n",
        "        # Build trigrams: sliding window of size N\n",
        "        ngrams = [tuple(tokens[i:i+N]) for i in range(len(tokens) - N + 1)]\n",
        "        log_prob = 0.0\n",
        "        for trigram in ngrams:\n",
        "            trigram_log_prob = calculate_trigram_log_probability(model, trigram, vocab)\n",
        "            log_prob += trigram_log_prob\n",
        "        # Number of predictions is len(tokens) - (N - 1)\n",
        "        num_predictions = len(tokens) - (N - 1)\n",
        "        perplexity = np.exp(-log_prob / num_predictions)\n",
        "        perplexities.append(perplexity)\n",
        "\n",
        "# Compute average perplexity over all sentences\n",
        "avg_perplexity = np.mean(perplexities)\n",
        "\n",
        "# Write results to file\n",
        "type_str = \"train\"\n",
        "with open(f\"2022102078_PP_FFNN_{N}_{type_str}_perplexity.txt\", 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"{avg_perplexity}\\n\")\n",
        "    # Write each sentence and its perplexity\n",
        "    # Assuming train_sentences corresponds 1-to-1 with computed perplexities:\n",
        "    for sentence, perp in zip(train_sentences, perplexities):\n",
        "        f.write(f\"{sentence}\\t{perp:.4f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage: compute perplexity for each sentence in train_sentences\n",
        "perplexities = []\n",
        "for sentence in test_sentences:\n",
        "    # Sentence may need to be split into sentences if it contains multiple sentences:\n",
        "    sentence_list = nltk.sent_tokenize(sentence)\n",
        "    # For simplicity, let's assume each 'sentence' is processed separately\n",
        "    for sent in sentence_list:\n",
        "        tokens = tokenizer([sent], N)  # tokenizer expects a list of sentences\n",
        "        # Build trigrams: sliding window of size N\n",
        "        ngrams = [tuple(tokens[i:i+N]) for i in range(len(tokens) - N + 1)]\n",
        "        log_prob = 0.0\n",
        "        for trigram in ngrams:\n",
        "            trigram_log_prob = calculate_trigram_log_probability(model, trigram, vocab)\n",
        "            log_prob += trigram_log_prob\n",
        "        # Number of predictions is len(tokens) - (N - 1)\n",
        "        num_predictions = len(tokens) - (N - 1)\n",
        "        perplexity = np.exp(-log_prob / num_predictions)\n",
        "        perplexities.append(perplexity)\n",
        "\n",
        "# Compute average perplexity over all sentences\n",
        "avg_perplexity = np.mean(perplexities)\n",
        "\n",
        "# Write results to file\n",
        "type_str = \"test\"\n",
        "with open(f\"2022102078_PP_FFNN_{N}_{type_str}_perplexity.txt\", 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"{avg_perplexity}\\n\")\n",
        "    # Write each sentence and its perplexity\n",
        "    # Assuming train_sentences corresponds 1-to-1 with computed perplexities:\n",
        "    for sentence, perp in zip(test_sentences, perplexities):\n",
        "        f.write(f\"{sentence}\\t{perp:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPzyqFlReBLv",
        "outputId": "bb55d4ef-2a36-4383-8b72-e9cbedd9f63f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 5.1530\n",
            "Epoch 2, Loss: 4.5082\n",
            "Epoch 3, Loss: 4.2363\n",
            "Epoch 4, Loss: 4.0518\n",
            "Epoch 5, Loss: 3.9134\n"
          ]
        }
      ],
      "source": [
        "class FivegramDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, target = self.data[idx]\n",
        "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "class FFNNLanguageModelFivegram(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=100, hidden_size = 50):\n",
        "        super(FFNNLanguageModelFivegram, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.fc1 = nn.Linear(embed_size * 4, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "        # self.fc3 = nn.Linear(hidden2, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten embedding\n",
        "        x = self.relu(self.fc1(x))\n",
        "        # x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.dropout(x)\n",
        "        # x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_dataset = FivegramDataset(encoded_dataset_5)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = FFNNLanguageModelFivegram(vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for context, target in train_loader:\n",
        "        context, target = context.to(device), target.to(device)  # Move to GPU\n",
        "        optimizer.zero_grad()\n",
        "        output = model(context)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pitgrpBsl0GF"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"PP_FFNN_5.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kag7Zkhl4Um"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "N = 5\n",
        "\n",
        "def calculate_5gram_log_probability(model, fgram, vocab):\n",
        "    \"\"\"\n",
        "    Compute the log probability for a single trigram.\n",
        "    trigram: a tuple of three tokens, e.g. (\"the\", \"cat\", \"sat\")\n",
        "    \"\"\"\n",
        "    # Since trigram is exactly 3 tokens, we can directly unpack:\n",
        "    w1, w2, w3, w4, w5 = fgram\n",
        "\n",
        "    # Replace OOV words with <UNK>\n",
        "    w1 = w1 if w1 in vocab else \"<UNK>\"\n",
        "    w2 = w2 if w2 in vocab else \"<UNK>\"\n",
        "    w3 = w3 if w3 in vocab else \"<UNK>\"\n",
        "    w4 = w4 if w4 in vocab else \"<UNK>\"\n",
        "    w5 = w5 if w5 in vocab else \"<UNK>\"\n",
        "\n",
        "    # Create context tensor from the first two words\n",
        "    context_tensor = torch.tensor([[vocab[w1], vocab[w2], vocab[w3], vocab[w4]]], dtype=torch.long).to(device)\n",
        "\n",
        "    # Forward pass: add batch dimension\n",
        "    output = model(context_tensor.unsqueeze(0))  # shape: (1, vocab_size)\n",
        "\n",
        "    # Convert logits to log probabilities\n",
        "    log_probs = F.log_softmax(output, dim=1)\n",
        "\n",
        "    # Get log probability for the actual next word\n",
        "    target_idx = vocab[w5]\n",
        "    word_log_prob = log_probs[0, target_idx].item()\n",
        "\n",
        "    return word_log_prob\n",
        "\n",
        "# Example usage: compute perplexity for each sentence in train_sentences\n",
        "perplexities = []\n",
        "for sentence in train_sentences:\n",
        "    # Sentence may need to be split into sentences if it contains multiple sentences:\n",
        "    sentence_list = nltk.sent_tokenize(sentence)\n",
        "    # For simplicity, let's assume each 'sentence' is processed separately\n",
        "    for sent in sentence_list:\n",
        "        tokens = tokenizer([sent], N)  # tokenizer expects a list of sentences\n",
        "        # Build trigrams: sliding window of size N\n",
        "        ngrams = [tuple(tokens[i:i+N]) for i in range(len(tokens) - N + 1)]\n",
        "        log_prob = 0.0\n",
        "        for fgram in ngrams:\n",
        "            fgram_log_prob = calculate_5gram_log_probability(model, fgram, vocab)\n",
        "            log_prob += fgram_log_prob\n",
        "        # Number of predictions is len(tokens) - (N - 1)\n",
        "        num_predictions = len(tokens) - (N - 1)\n",
        "        perplexity = np.exp(-log_prob / num_predictions)\n",
        "        perplexities.append(perplexity)\n",
        "\n",
        "# Compute average perplexity over all sentences\n",
        "avg_perplexity = np.mean(perplexities)\n",
        "\n",
        "\n",
        "# Write results to file\n",
        "type_str = \"train\"\n",
        "with open(f\"2022102078_PP_FFNN_{N}_{type_str}_perplexity.txt\", 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"{avg_perplexity}\\n\")\n",
        "    # Write each sentence and its perplexity\n",
        "    # Assuming train_sentences corresponds 1-to-1 with computed perplexities:\n",
        "    for sentence, perp in zip(train_sentences, perplexities):\n",
        "        f.write(f\"{sentence}\\t{perp:.4f}\\n\")\n",
        "\n",
        "\n",
        "# Example usage: compute perplexity for each sentence in train_sentences\n",
        "perplexities = []\n",
        "for sentence in test_sentences:\n",
        "    # Sentence may need to be split into sentences if it contains multiple sentences:\n",
        "    sentence_list = nltk.sent_tokenize(sentence)\n",
        "    # For simplicity, let's assume each 'sentence' is processed separately\n",
        "    for sent in sentence_list:\n",
        "        tokens = tokenizer([sent], N)  # tokenizer expects a list of sentences\n",
        "        # Build trigrams: sliding window of size N\n",
        "        ngrams = [tuple(tokens[i:i+N]) for i in range(len(tokens) - N + 1)]\n",
        "        log_prob = 0.0\n",
        "        for fgram in ngrams:\n",
        "            fgram_log_prob = calculate_5gram_log_probability(model, fgram, vocab)\n",
        "            log_prob += fgram_log_prob\n",
        "        # Number of predictions is len(tokens) - (N - 1)\n",
        "        num_predictions = len(tokens) - (N - 1)\n",
        "        perplexity = np.exp(-log_prob / num_predictions)\n",
        "        perplexities.append(perplexity)\n",
        "\n",
        "# Compute average perplexity over all sentences\n",
        "avg_perplexity = np.mean(perplexities)\n",
        "\n",
        "\n",
        "# Write results to file\n",
        "type_str = \"test\"\n",
        "with open(f\"2022102078_PP_FFNN_{N}_{type_str}_perplexity.txt\", 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"{avg_perplexity}\\n\")\n",
        "    # Write each sentence and its perplexity\n",
        "    # Assuming train_sentences corresponds 1-to-1 with computed perplexities:\n",
        "    for sentence, perp in zip(test_sentences, perplexities):\n",
        "        f.write(f\"{sentence}\\t{perp:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Opw_WX_znd-c"
      },
      "outputs": [],
      "source": [
        "# Define a Dataset for our token sequences\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        \"\"\"\n",
        "        sequences: list of (input_seq, target_seq) pairs,\n",
        "                   where each sequence is a list of token indices.\n",
        "        \"\"\"\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq, target_seq = self.sequences[idx]\n",
        "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
        "\n",
        "# Define the vanilla RNN language model\n",
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1):\n",
        "        super(RNNLanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        # Use vanilla RNN here\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        # Fully connected layer to map RNN outputs to vocabulary logits\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x shape: (batch_size, seq_length)\n",
        "        embed = self.embedding(x)            # (batch_size, seq_length, embed_dim)\n",
        "        output, hidden = self.rnn(embed, hidden)  # (batch_size, seq_length, hidden_dim)\n",
        "        logits = self.fc(output)             # (batch_size, seq_length, vocab_size)\n",
        "        return logits, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Qj5bi2MnuNg",
        "outputId": "50339a81-4ff1-4312-a48a-7eb0a7b9bb89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 6.1266\n",
            "Epoch [2/10], Loss: 4.6528\n",
            "Epoch [3/10], Loss: 4.4081\n",
            "Epoch [4/10], Loss: 4.2721\n",
            "Epoch [5/10], Loss: 4.1744\n",
            "Epoch [6/10], Loss: 4.0957\n",
            "Epoch [7/10], Loss: 4.0286\n",
            "Epoch [8/10], Loss: 3.9706\n",
            "Epoch [9/10], Loss: 3.9344\n",
            "Epoch [10/10], Loss: 3.9072\n"
          ]
        }
      ],
      "source": [
        "vocab_size    = len(vocab)\n",
        "embed_dim     = 200   # Embedding dimensionality\n",
        "hidden_dim    = 50    # Hidden state dimensionality\n",
        "num_layers    = 1\n",
        "batch_size    = 128\n",
        "num_epochs    = 10\n",
        "learning_rate = 0.001\n",
        "accumulation_steps = 4  # Update every 4 batches\n",
        "\n",
        "def create_sequences(tokens, seq_length):\n",
        "    sequences = []\n",
        "    for i in range(0, len(tokens) - seq_length):\n",
        "        input_seq = tokens[i:i+seq_length]\n",
        "        target_seq = tokens[i+1:i+seq_length+1]  # shifted by one\n",
        "        sequences.append((input_seq, target_seq))\n",
        "    return sequences\n",
        "\n",
        "# Example usage:\n",
        "seq_length = 50  # or any other fixed length suitable for your model/memory\n",
        "sequences = create_sequences(tokens, seq_length)\n",
        "\n",
        "def encode_sequence(sequences, vocab, unk_words):\n",
        "    encoded_sequences = []\n",
        "    for input_seq, target_seq in sequences:\n",
        "        # Encode input sequence: replace rare words with <UNK> and map to indices\n",
        "        encoded_input = [\n",
        "            vocab[token] if token not in unk_words else vocab[\"<UNK>\"]\n",
        "            for token in input_seq\n",
        "        ]\n",
        "        # Encode target sequence similarly\n",
        "        encoded_target = [\n",
        "            vocab[token] if token not in unk_words else vocab[\"<UNK>\"]\n",
        "            for token in target_seq\n",
        "        ]\n",
        "        encoded_sequences.append((encoded_input, encoded_target))\n",
        "    return encoded_sequences\n",
        "\n",
        "encoded_sequences = encode_sequence(sequences, vocab, unk_words)\n",
        "# Assume `encoded_sequences` is produced by your updated encode_dataset function.\n",
        "# Each element in encoded_sequences is a tuple: (input_seq, target_seq)\n",
        "dataset = TextDataset(encoded_sequences)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = RNNLanguageModel(vocab_size, embed_dim, hidden_dim, num_layers).to(device)\n",
        "\n",
        "# Use SparseAdam if using nn.Embedding\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "# Training loop using teacher forcing\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for i, (batch_inputs, batch_targets) in enumerate(dataloader):\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_targets = batch_targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed Precision Forward Pass\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            logits, _ = model(batch_inputs)  # logits: (batch_size, seq_length, vocab_size)\n",
        "            logits = logits.view(-1, vocab_size)       # Shape: (batch_size * seq_length, vocab_size)\n",
        "            batch_targets = batch_targets.view(-1)     # Shape: (batch_size * seq_length)\n",
        "            loss = F.cross_entropy(logits, batch_targets) / accumulation_steps\n",
        "\n",
        "        # Backward pass and optimizer step\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient Accumulation Step\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            # Gradient Clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.detach().item() * accumulation_steps  # Scale back the accumulated loss\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OokQPUJkunI7"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"PP_RNN.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEIX7LY3u29L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def calculate_sentence_probability(model, sentence, vocab, unk_words, device='cpu'):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Tokenize and encode the sentence\n",
        "        tokens = tokenizer(sentence, N)  # Split sentence into tokens\n",
        "        # print(tokens)\n",
        "        if len(tokens) < 2:\n",
        "            return 0.0  # Probability of a single word or empty sentence is undefined or zero\n",
        "\n",
        "        # Convert tokens to indices\n",
        "        encoded_sentence = [\n",
        "            vocab[token] if token in vocab else vocab[\"<UNK>\"]\n",
        "            for token in tokens\n",
        "        ]\n",
        "        # print(encoded_sentence)\n",
        "        # Prepare input and target sequences\n",
        "        input_seq = torch.tensor(encoded_sentence[:-1], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        target_seq = torch.tensor(encoded_sentence[1:], dtype=torch.long).to(device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        logits, _ = model(input_seq)  # logits shape: (1, seq_length, vocab_size)\n",
        "\n",
        "        # Calculate probabilities using softmax\n",
        "        log_probs = F.log_softmax(logits, dim=-1)  # (1, seq_length, vocab_size)\n",
        "\n",
        "        # Get the log probabilities of the target words\n",
        "        target_log_probs = log_probs[0, torch.arange(len(target_seq)), target_seq]\n",
        "        log_prob_sentence = torch.sum(target_log_probs)\n",
        "        # print(log_prob_sentence)\n",
        "        num_predictions = len(encoded_sentence) - 1\n",
        "        # print(num_predictions)\n",
        "        # Convert to probability\n",
        "        # sentence_prob = torch.exp(log_prob_sentence).item()\n",
        "\n",
        "    return log_prob_sentence, num_predictions\n",
        "\n",
        "\n",
        "perplexities = []\n",
        "for sentence in train_sentences:\n",
        "    # Sentence may need to be split into sentences if it contains multiple sentences:\n",
        "    sentence_list = nltk.sent_tokenize(sentence)\n",
        "    # For simplicity, let's assume each 'sentence' is processed separately\n",
        "    log_prob, num_predictions = calculate_sentence_probability(model, sentence_list, vocab, unk_words, device)\n",
        "    perplexity = np.exp(-log_prob.cpu().item() / num_predictions)\n",
        "    perplexities.append(perplexity)\n",
        "\n",
        "# Compute average perplexity over all sentences\n",
        "avg_perplexity = np.mean(perplexities)\n",
        "\n",
        "# Write results to file\n",
        "type_str = \"train\"\n",
        "with open(f\"2022102078_PP_RNN_{type_str}_perplexity.txt\", 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"{avg_perplexity}\\n\")\n",
        "    # Write each sentence and its perplexity\n",
        "    # Assuming train_sentences corresponds 1-to-1 with computed perplexities:\n",
        "    for sentence, perp in zip(train_sentences, perplexities):\n",
        "        f.write(f\"{sentence}\\t{perp:.4f}\\n\")\n",
        "\n",
        "perplexities = []\n",
        "for sentence in test_sentences:\n",
        "    # Sentence may need to be split into sentences if it contains multiple sentences:\n",
        "    sentence_list = nltk.sent_tokenize(sentence)\n",
        "    # For simplicity, let's assume each 'sentence' is processed separately\n",
        "    log_prob, num_predictions = calculate_sentence_probability(model, sentence_list, vocab, unk_words, device)\n",
        "    perplexity = np.exp(-log_prob.cpu().item() / num_predictions)\n",
        "    perplexities.append(perplexity)\n",
        "\n",
        "# Compute average perplexity over all sentences\n",
        "avg_perplexity = np.mean(perplexities)\n",
        "\n",
        "# Write results to file\n",
        "type_str = \"test\"\n",
        "with open(f\"2022102078_PP_RNN_{type_str}_perplexity.txt\", 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"{avg_perplexity}\\n\")\n",
        "    # Write each sentence and its perplexity\n",
        "    # Assuming train_sentences corresponds 1-to-1 with computed perplexities:\n",
        "    for sentence, perp in zip(test_sentences, perplexities):\n",
        "        f.write(f\"{sentence}\\t{perp:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoMX3r6nwy7l"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1):\n",
        "        super(LSTMLanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        # Use LSTM here\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        # Fully connected layer to map LSTM outputs to vocabulary logits\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x shape: (batch_size, seq_length)\n",
        "        embed = self.embedding(x)               # (batch_size, seq_length, embed_dim)\n",
        "        output, hidden = self.lstm(embed, hidden)  # (batch_size, seq_length, hidden_dim)\n",
        "        logits = self.fc(output)                # (batch_size, seq_length, vocab_size)\n",
        "        return logits, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize hidden state and cell state to zeros\n",
        "        # Hidden and cell state shapes: (num_layers, batch_size, hidden_dim)\n",
        "        return (torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(next(self.parameters()).device),\n",
        "                torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(next(self.parameters()).device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-wszc7QxTFl",
        "outputId": "3a7f4f70-5aca-4aa9-ba9e-9eb490919f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 5.4902\n",
            "Epoch [2/10], Loss: 4.6517\n",
            "Epoch [3/10], Loss: 4.3664\n",
            "Epoch [4/10], Loss: 4.2288\n",
            "Epoch [5/10], Loss: 4.1477\n",
            "Epoch [6/10], Loss: 4.0897\n",
            "Epoch [7/10], Loss: 4.0460\n",
            "Epoch [8/10], Loss: 4.0109\n",
            "Epoch [9/10], Loss: 3.9823\n",
            "Epoch [10/10], Loss: 3.9574\n"
          ]
        }
      ],
      "source": [
        "vocab_size    = len(vocab)\n",
        "embed_dim     = 200   # Embedding dimensionality\n",
        "hidden_dim    = 10   # Hidden state dimensionality\n",
        "num_layers    = 1\n",
        "batch_size    = 64\n",
        "num_epochs    = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = LSTMLanguageModel(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "# DataLoader for batching\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
        "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
        "\n",
        "        # Initialize hidden state\n",
        "        batch_size_actual = input_seq.size(0)\n",
        "        hidden = model.init_hidden(batch_size_actual)\n",
        "\n",
        "        # Detach hidden state to prevent backprop through entire history\n",
        "        hidden = tuple(h.detach() for h in hidden)\n",
        "\n",
        "        # Forward pass\n",
        "        logits, hidden = model(input_seq, hidden)\n",
        "\n",
        "        # Reshape logits and targets for CrossEntropyLoss\n",
        "        logits = logits.view(-1, vocab_size)   # (batch_size * seq_length, vocab_size)\n",
        "        target_seq = target_seq.view(-1)       # (batch_size * seq_length)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(logits, target_seq)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (optional but recommended)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7-vfczRx4En"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"PP_LSTM.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L04lvgxHx8T3"
      },
      "outputs": [],
      "source": [
        "def calculate_sentence_probability_lstm(model, sentence, vocab, unk_words, device='cpu'):\n",
        "    \"\"\"\n",
        "    Calculate the log probability of a given sentence using an LSTM language model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained LSTM language model.\n",
        "        sentence (str): The input sentence whose probability is to be calculated.\n",
        "        vocab (dict): Mapping from tokens to indices.\n",
        "        unk_words (list): List of words to be replaced with <UNK> for robustness.\n",
        "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        log_prob_sentence (float): Log probability of the sentence.\n",
        "        num_predictions (int): Number of predictions made (for calculating average log prob).\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Tokenize and encode the sentence\n",
        "        tokens = tokenizer(sentence, N) # Adjust if using a custom tokenizer\n",
        "        if len(tokens) < 2:\n",
        "            return 0.0, 0  # Probability of a single word or empty sentence is undefined or zero\n",
        "\n",
        "        # Convert tokens to indices, mapping unknown words to \"<UNK>\"\n",
        "        encoded_sentence = [\n",
        "            vocab[token] if token in vocab else vocab[\"<UNK>\"]\n",
        "            for token in tokens\n",
        "        ]\n",
        "\n",
        "        # Prepare input and target sequences\n",
        "        input_seq = torch.tensor(encoded_sentence[:-1], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        target_seq = torch.tensor(encoded_sentence[1:], dtype=torch.long).to(device)\n",
        "\n",
        "        # Initialize hidden and cell states for LSTM\n",
        "        batch_size = input_seq.size(0)\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        logits, _ = model(input_seq, hidden)  # logits shape: (1, seq_length, vocab_size)\n",
        "\n",
        "        # Calculate log probabilities using softmax\n",
        "        log_probs = F.log_softmax(logits, dim=-1)  # (1, seq_length, vocab_size)\n",
        "\n",
        "        # Get the log probabilities of the target words\n",
        "        target_log_probs = log_probs[0, torch.arange(len(target_seq)), target_seq]\n",
        "        log_prob_sentence = torch.sum(target_log_probs).item()\n",
        "\n",
        "        # Count the number of predictions\n",
        "        num_predictions = len(encoded_sentence) - 1\n",
        "\n",
        "    return log_prob_sentence, num_predictions\n",
        "\n",
        "\n",
        "\n",
        "perplexities = []\n",
        "for sentence in train_sentences:\n",
        "    # Sentence may need to be split into sentences if it contains multiple sentences:\n",
        "    sentence_list = nltk.sent_tokenize(sentence)\n",
        "    # For simplicity, let's assume each 'sentence' is processed separately\n",
        "    log_prob, num_predictions = calculate_sentence_probability_lstm(model, sentence_list, vocab, unk_words, device)\n",
        "    perplexity = np.exp(-log_prob / num_predictions)\n",
        "    perplexities.append(perplexity)\n",
        "\n",
        "# Compute average perplexity over all sentences\n",
        "avg_perplexity = np.mean(perplexities)\n",
        "\n",
        "\n",
        "# Write results to file\n",
        "type_str = \"train\"\n",
        "with open(f\"2022102078_PP_LSTM_{type_str}_perplexity.txt\", 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"{avg_perplexity}\\n\")\n",
        "    # Write each sentence and its perplexity\n",
        "    # Assuming train_sentences corresponds 1-to-1 with computed perplexities:\n",
        "    for sentence, perp in zip(train_sentences, perplexities):\n",
        "        f.write(f\"{sentence}\\t{perp:.4f}\\n\")\n",
        "\n",
        "\n",
        "perplexities = []\n",
        "for sentence in test_sentences:\n",
        "    # Sentence may need to be split into sentences if it contains multiple sentences:\n",
        "    sentence_list = nltk.sent_tokenize(sentence)\n",
        "    # For simplicity, let's assume each 'sentence' is processed separately\n",
        "    log_prob, num_predictions = calculate_sentence_probability_lstm(model, sentence_list, vocab, unk_words, device)\n",
        "    perplexity = np.exp(-log_prob / num_predictions)\n",
        "    perplexities.append(perplexity)\n",
        "\n",
        "# Compute average perplexity over all sentences\n",
        "avg_perplexity = np.mean(perplexities)\n",
        "\n",
        "# Write results to file\n",
        "type_str = \"test\"\n",
        "with open(f\"2022102078_PP_LSTM_{type_str}_perplexity.txt\", 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"{avg_perplexity}\\n\")\n",
        "    # Write each sentence and its perplexity\n",
        "    # Assuming train_sentences corresponds 1-to-1 with computed perplexities:\n",
        "    for sentence, perp in zip(test_sentences, perplexities):\n",
        "        f.write(f\"{sentence}\\t{perp:.4f}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
